{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport string\nimport nltk\nnltk.download('punkt')\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:56:05.273486Z","iopub.execute_input":"2025-11-03T06:56:05.274005Z","iopub.status.idle":"2025-11-03T06:56:13.223602Z","shell.execute_reply.started":"2025-11-03T06:56:05.273983Z","shell.execute_reply":"2025-11-03T06:56:13.222985Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/flickr8k\"\n\n# Load captions\ncaptions_file = os.path.join(DATA_DIR, \"captions.txt\")\ncaptions_df = pd.read_csv(captions_file)\ncaptions_df.head()\n\n# Clean captions\ndef clean_caption(text):\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    tokens = nltk.word_tokenize(text)\n    tokens = [word for word in tokens if word.isalpha()]\n    return \" \".join(tokens)\n\ncaptions_df['caption'] = captions_df['caption'].apply(clean_caption)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:56:21.304121Z","iopub.execute_input":"2025-11-03T06:56:21.304810Z","iopub.status.idle":"2025-11-03T06:56:24.049798Z","shell.execute_reply.started":"2025-11-03T06:56:21.304787Z","shell.execute_reply":"2025-11-03T06:56:24.049160Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"all_captions = captions_df['caption'].tolist()\n\nword_freq = {}\nfor cap in all_captions:\n    for word in cap.split():\n        word_freq[word] = word_freq.get(word, 0) + 1\n\n\nwords = [w for w in word_freq if word_freq[w] >= 5]\n\n\nword2idx = {w:i+4 for i,w in enumerate(words)}\nword2idx[\"<PAD>\"] = 0\nword2idx[\"<SOS>\"] = 1\nword2idx[\"<EOS>\"] = 2\nword2idx[\"<UNK>\"] = 3\n\nidx2word = {i:w for w,i in word2idx.items()}\nvocab_size = len(word2idx)\nvocab_size\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:56:27.496955Z","iopub.execute_input":"2025-11-03T06:56:27.497744Z","iopub.status.idle":"2025-11-03T06:56:27.626658Z","shell.execute_reply.started":"2025-11-03T06:56:27.497711Z","shell.execute_reply":"2025-11-03T06:56:27.625952Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"2988"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nresnet = models.resnet50(pretrained=True)\nresnet = nn.Sequential(*list(resnet.children())[:-1])  # remove final FC layer\nresnet = resnet.to(device)\nresnet.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\ndef extract_features(image_path):\n    img = Image.open(image_path).convert(\"RGB\")\n    img = transform(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        features = resnet(img)\n    return features.squeeze().cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:56:31.341252Z","iopub.execute_input":"2025-11-03T06:56:31.341757Z","iopub.status.idle":"2025-11-03T06:56:32.786474Z","shell.execute_reply.started":"2025-11-03T06:56:31.341737Z","shell.execute_reply":"2025-11-03T06:56:32.785660Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 160MB/s] \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, df, img_dir):\n        self.df = df\n        self.img_dir = img_dir\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['image'])\n        caption = row['caption'].split()\n\n        \n        encoded = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in caption]\n        encoded = [word2idx[\"<SOS>\"]] + encoded + [word2idx[\"<EOS>\"]]\n\n        \n        caption_tensor = torch.tensor(encoded, dtype=torch.long)\n        img_features = torch.tensor(extract_features(img_path), dtype=torch.float32)\n\n        return img_features, caption_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:56:37.616489Z","iopub.execute_input":"2025-11-03T06:56:37.616749Z","iopub.status.idle":"2025-11-03T06:56:37.622383Z","shell.execute_reply.started":"2025-11-03T06:56:37.616733Z","shell.execute_reply":"2025-11-03T06:56:37.621689Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class CaptionDecoder(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size + 2048, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, img_feat, captions):\n        img_feat = img_feat.unsqueeze(1)  \n        embeddings = self.embed(captions) \n\n        \n        img_feat = img_feat.repeat(1, embeddings.size(1), 1)\n\n        lstm_input = torch.cat((img_feat, embeddings), dim=2)\n        out, _ = self.lstm(lstm_input)\n        out = self.fc(out)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:56:40.719565Z","iopub.execute_input":"2025-11-03T06:56:40.720157Z","iopub.status.idle":"2025-11-03T06:56:40.725418Z","shell.execute_reply.started":"2025-11-03T06:56:40.720135Z","shell.execute_reply":"2025-11-03T06:56:40.724692Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset = FlickrDataset(captions_df, os.path.join(DATA_DIR, \"Images\"))\nloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)\n\nmodel = CaptionDecoder(256, 256, vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<PAD>\"])\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:56:44.447738Z","iopub.execute_input":"2025-11-03T06:56:44.448272Z","iopub.status.idle":"2025-11-03T06:56:44.564002Z","shell.execute_reply.started":"2025-11-03T06:56:44.448250Z","shell.execute_reply":"2025-11-03T06:56:44.563419Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"for epoch in range(2):  \n    total_loss = 0\n    for batch in tqdm(loader):\n        imgs, caps = zip(*batch)\n        imgs = torch.stack(imgs).to(device)\n        caps = torch.nn.utils.rnn.pad_sequence(caps, batch_first=True, padding_value=word2idx[\"<PAD>\"]).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs, caps[:,:-1])   \n        loss = criterion(outputs.reshape(-1, vocab_size), caps[:,1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T06:56:47.841251Z","iopub.execute_input":"2025-11-03T06:56:47.841553Z","iopub.status.idle":"2025-11-03T07:16:16.947899Z","shell.execute_reply.started":"2025-11-03T06:56:47.841536Z","shell.execute_reply":"2025-11-03T07:16:16.947340Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1265/1265 [09:53<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 3.9481\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1265/1265 [09:35<00:00,  2.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 3.2534\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def generate_caption(image_path, max_len=20):\n    model.eval()\n    feat = torch.tensor(extract_features(image_path)).float().to(device)\n\n    caption = [word2idx[\"<SOS>\"]]\n\n    for _ in range(max_len):\n        inp = torch.tensor(caption).unsqueeze(0).to(device)\n        out = model(feat.unsqueeze(0), inp)\n        next_word = out.argmax(2)[:,-1].item()\n        caption.append(next_word)\n        if next_word == word2idx[\"<EOS>\"]:\n            break\n\n    return \" \".join(idx2word[w] for w in caption if w not in (0,1,2,3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T07:16:27.158441Z","iopub.execute_input":"2025-11-03T07:16:27.159025Z","iopub.status.idle":"2025-11-03T07:16:27.163905Z","shell.execute_reply.started":"2025-11-03T07:16:27.159003Z","shell.execute_reply":"2025-11-03T07:16:27.163266Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"test_image = \"/kaggle/input/flickr8k/Images/1000268201_693b08cb0e.jpg\"\nprint(generate_caption(test_image))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T07:16:31.919219Z","iopub.execute_input":"2025-11-03T07:16:31.919692Z","iopub.status.idle":"2025-11-03T07:16:31.953112Z","shell.execute_reply.started":"2025-11-03T07:16:31.919669Z","shell.execute_reply":"2025-11-03T07:16:31.952548Z"}},"outputs":[{"name":"stdout","text":"a man in a blue shirt is sitting on a bench\n","output_type":"stream"}],"execution_count":16}]}